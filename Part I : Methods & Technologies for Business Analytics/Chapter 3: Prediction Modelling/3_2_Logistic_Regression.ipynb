{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "3.2.Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITU-Business-Analytics-Team/Business_Analytics_for_Professionals/blob/main/Part%20I%20%3A%20Methods%20%26%20Technologies%20for%20Business%20Analytics/Chapter%203%3A%20Prediction%20Modelling/3_2_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOIuoNDPnvFP"
      },
      "source": [
        "# **Prediction Modelling**\n",
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXfDnweNun4G"
      },
      "source": [
        "### **Classifying Breast Cancer Tumors**\n",
        "\n",
        "In this exercise, there is a dataset that containts breast biopsy results for 569 tumors. For each tumor 30 numeric value is measured including:\n",
        "\n",
        "- radius (mean of distances from center to points on the perimeter)\n",
        "- texture (standard deviation of gray-scale values)\n",
        "- perimeter\n",
        "- area\n",
        "- smoothness (local variation in radius lengths)\n",
        "- compactness (perimeter^2 / area - 1.0)\n",
        "- concavity (severity of concave portions of the contour)\n",
        "- concave points (number of concave portions of the contour)\n",
        "- symmetry\n",
        "- fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The class labels are Malignant ('M') indicating a dangerous tumor vs Benign ('B') indicating a non-cancerous tissue. A Logistic Regression model is trained to classify tumors either malignant or benign when biopsy measurements are provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4gR631Eun4K"
      },
      "source": [
        "```sklearn.datasets``` library have built-in functions to import many of the well-known datasets. The wisconsin breast cancer dataset can be imported by using one of those built-in functions: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-b2R6mTAfX1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY36gCX6un4L"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "dataset = load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl1BK441un4N"
      },
      "source": [
        "All datasets imported via ```sklearn.datasets``` comes as a dictionary with 7 keys:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mR_kr2Aun4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bb265c-dd53-4217-aa2d-5faff1352b10"
      },
      "source": [
        "dataset.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtQcVNBPun4O"
      },
      "source": [
        "These keys carry the following information:\n",
        "- **data**: Contains the data for all features ($X$) as a numpy matrix. Each column corresponds to one feature.\n",
        "- **target**: Contains the data for the target variable ($y$) a numpy matrix/array\n",
        "- **frame**: Contains all data as a pandas dataframe. Created only if ```as_frame=True```. You will have the information form \"data\", \"target\", \"target_names\" and \"feature_names\" keys in a single dataframe.\n",
        "- **target_names**: The names of target classes\n",
        "- **DESCR**: A detailed description of the dataset.\n",
        "- **feature_names**: Names of each feature as a list\n",
        "- **filename**: The path to the location of the data\n",
        "\n",
        "Let's print the description of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "Czkdvjnp4DA-",
        "outputId": "80ad4a5c-e06c-4a12-cea9-5d208e971bb6"
      },
      "source": [
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcQ22fP8un4P"
      },
      "source": [
        "#print(dataset['DESCR'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIDIoXbUun4Q"
      },
      "source": [
        "Let's work with the pandas dataframe and print first 5 rows to see the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgjqULBmun4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd07f7b-0831-4e83-ba3c-f0d28afb4eaa"
      },
      "source": [
        "num_benign = sum(dataset.target==1)\n",
        "num_malignant = sum(dataset.target == 0)\n",
        "\n",
        "print(\"Number of malignant tumors:\", num_malignant)\n",
        "print(\"Number of benign tumors:\", num_benign)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of malignant tumors: 212\n",
            "Number of benign tumors: 357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cenv1MJ4un4S"
      },
      "source": [
        "- It can be seen that malignant tumors are marked to be 0 and benign tumors are marked with value 1 in the target column. By using the code above, number of malignant tumors is 212 adn number of benign tumors is 357. This is consistent with the information given in the DESCR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTcIMDtR6cHR"
      },
      "source": [
        "X=df\n",
        "y= pd.DataFrame(dataset.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "d7ePfo1N7NX9",
        "outputId": "dfdd737e-e490-4adc-b8f2-29f68b2f1ff8"
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              "0        17.99         10.38  ...          0.4601                  0.11890\n",
              "1        20.57         17.77  ...          0.2750                  0.08902\n",
              "2        19.69         21.25  ...          0.3613                  0.08758\n",
              "3        11.42         20.38  ...          0.6638                  0.17300\n",
              "4        20.29         14.34  ...          0.2364                  0.07678\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "JS0BvncU7R4I",
        "outputId": "ae025104-cfd5-40a5-f939-4794e49ec924"
      },
      "source": [
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0\n",
              "0  0\n",
              "1  0\n",
              "2  0\n",
              "3  0\n",
              "4  0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NlKQss2un4T"
      },
      "source": [
        "- Now y dataframe has the target data and X has the all other information. Let's go ahead and check the performance of Logistic Regression model on this dataset. \n",
        "- 10-fold cross validation will be performed in which the whole data divided into 10 subsets. \n",
        "    - Then model is trained with 9 of those subsets and tested with the remaining subset. \n",
        "    - This process is repeated 10 times for each the test set is changed\n",
        "    - This means 10 models are trained and get 10 test results\n",
        "    - Average of these results will give us a better understanding about the performance\n",
        "-  ```sklearn.model_selection.cross_val_score``` can be used for running a cross validation\n",
        "- Logistic regression estimator of sklearn can be found in ```sklearn.linear_model.LogisticRegression```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJBUZEFpun4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a88e759-8ffd-4a0a-ae65-d354dbbb3b28"
      },
      "source": [
        "#import required functions\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#create the estimator. increased the max_iter to 5000 so that the optimal result is converged.\n",
        "#Exercise: Play with max_iter parameter to see if it affects the results\n",
        "lr = LogisticRegression(max_iter =5000)\n",
        "\n",
        "#call cross_val_score with cv=10 for 10-fold cross validation\n",
        "scores = cross_val_score(lr,X,y,cv=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qKC7tJ6un4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cce97d-afdb-4f66-9cec-659e0fb91f73"
      },
      "source": [
        "print(\"Accuracy scores from all 10 runs are:\", scores)\n",
        "print(\"Average accuracy:\", scores.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores from all 10 runs are: [0.98245614 0.9122807  0.92982456 0.94736842 0.98245614 0.98245614\n",
            " 0.92982456 0.94736842 0.96491228 0.96428571]\n",
            "Average accuracy: 0.9543233082706767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmGsqOzdun4U"
      },
      "source": [
        "- There is a very high accuracy of 95\\%. Let's see the class specific metrics. In order to look at those, the data has to be mannually divided into training and test sets.\n",
        "- ```sklearn.model_selection.train_test_split``` function can be used for this purpose. \n",
        "- It has to be made sure that class proportions are conserved both in train and test sets. For this purpose, ```stratify = y``` option is used. This makes sure that proportion of values in y are preserved.\n",
        "- Let's spare 30% of our data for test and use the other 70% for training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrCT8f0Wun4V"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Since the data is shuffled, it is good practice to set a fixed random_state\n",
        "# so that the same train is obtained and test datasets when the code is rerun.\n",
        "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size= 0.3, shuffle = True, stratify = y, random_state = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_eS07lVun4V"
      },
      "source": [
        "### **Explicitly Training a Logistic Regression Model**\n",
        "\n",
        "- In order to train a logistic regression model, ```LogisticRegression.fit(...)``` function is called. \n",
        "- ```fit(...)``` function requires both the data for explanatory variables ($X$) values and the corresponding target values ($y$) values.\n",
        "- After the fit function finishes execution, LogisticRegression object will store optimal coefficient values inside.\n",
        "- Predictions can be created by using ```LogisticRegression.predict(...)``` once fit function finishes the execution.\n",
        "- With ```lr.coef_``` variable, the coefficients can be accessed if needed as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoqoDPjFun4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bf2338-e2fe-4167-f9cf-00bb6dfaa822"
      },
      "source": [
        "#train the model with the train data\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPhX6CYRun4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb9df79-22c4-4479-9397-89b9d6acc5bf"
      },
      "source": [
        "lr.coef_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.06086999,  0.24025603, -0.43483962,  0.04338172, -0.13346079,\n",
              "        -0.23470798, -0.44424919, -0.23872779, -0.13056251, -0.03428226,\n",
              "        -0.10025055,  0.3255418 ,  0.97555489, -0.1598955 , -0.01805033,\n",
              "        -0.00585793, -0.06200626, -0.03117806, -0.03234458, -0.00258271,\n",
              "         0.1035704 , -0.52294154, -0.19886095, -0.01561865, -0.22206243,\n",
              "        -0.52902115, -0.90516707, -0.42120402, -0.41615563, -0.07414351]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoTZFhagun4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625bca48-3f0e-4740-95b2-4cb30d7e3f1b"
      },
      "source": [
        "# Score functions for classifiers automatically calculates the accuracy\n",
        "accuracy_train = lr.score(X_train, y_train)\n",
        "accuracy_test = lr.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy on the train set:\", accuracy_train)\n",
        "print(\"Accuracy on the test set:\", accuracy_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the train set: 0.9748743718592965\n",
            "Accuracy on the test set: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFY5kkEtun4X"
      },
      "source": [
        "Usually training accuracy is expected to be higher than the test accuracy which is the case here as well. On the other hand, a big gap between train and test accuracies is not a good sign. If there is a big gap, this is an indication of overfitting. The difference observed here is an acceptable one.\n",
        "\n",
        "Now, it is time to check class-specific metrics. ```sklearn.metrics.classification_report``` creates most of the metrics of interest at once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEeogWyXun4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b83cc5-8aaf-4e2d-bc07-846ac13afda8"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# In order to use the classification report, the model's predictions need to be obtained first:\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "#Create and pring the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.92      0.93        64\n",
            "           1       0.95      0.96      0.96       107\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BzFbdMQun4X"
      },
      "source": [
        "- It can be seen that both malignant (0) and benign(1) cases have high values for precision, recall and f1-score. However, it seems that our model have relatively harder time identifying malignant tumors as precision, recall and f1-score are lower accross the board when compared to benign cases. \n",
        "- In order to see the numbers of correctly and incorrectly classified cases,  the ```sklearn.metrics.confusion_matrix``` function can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7HEUdG6un4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00d066a-38e2-43b1-fe0c-874241ec15dd"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#create and print confusion matrix\n",
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 59   5]\n",
            " [  4 103]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1HD01Cmun4Y"
      },
      "source": [
        "- First row corresponds to actual malignant cases in the data. Our model classified 59 of them correctly as malignant and 5 of them as benign\n",
        "- The second row of the matrix corresponds to actual benign cases. Our model correctly classified 103 of them as benign and 4 of them misclassified as malignant. \n",
        "- Although confusion matrix is a useful tool, it is not easy to read and digest.\n",
        "- A more visual version of this is the ```sklearn.metrics.plot_confusion_matrix``` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwdNI3fnun4Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "d1b82482-3fd6-4d76-8dff-fecd39884d8d"
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "plot_confusion_matrix(lr,X_test,y_test, display_labels= [\"Malignant\", \"Benign\"], cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fcfddac8650>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l17AApQhun4Y"
      },
      "source": [
        "### **Getting the probabilities used for classification**\n",
        "- ```LogisticRegression.predict_proba(...)``` function returns the probabilities of an instance belonging to each class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZXwNsJ3un4Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ba4f73fc-37a7-4247-de1c-27af2bde1f24"
      },
      "source": [
        "import pandas as pd\n",
        "#get the probabilities\n",
        "probs = lr.predict_proba(X_test)\n",
        "\n",
        "#put them into a dataframe (optional, for display purposes)\n",
        "df_probs = pd.DataFrame(probs, columns=[\"Prob Malignant (0)\", \"Prob Benign (1)\"])\n",
        "\n",
        "#add predicted class as a separate column\n",
        "df_probs[\"predicted class\"] = y_pred\n",
        "\n",
        "# check the top rows\n",
        "df_probs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prob Malignant (0)</th>\n",
              "      <th>Prob Benign (1)</th>\n",
              "      <th>predicted class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.005513</td>\n",
              "      <td>0.994487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.999983</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007656</td>\n",
              "      <td>0.992344</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.999948</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Prob Malignant (0)  Prob Benign (1)  predicted class\n",
              "0            0.005513         0.994487                1\n",
              "1            0.000017         0.999983                1\n",
              "2            0.998666         0.001334                0\n",
              "3            0.007656         0.992344                1\n",
              "4            0.000052         0.999948                1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UieBisEPun4Z"
      },
      "source": [
        "As seen above, whichever probability is higher, the instance is assigned to that class. \n",
        "\n",
        "### **Creating an ROC Curve and calculating AUC**\n",
        "\n",
        "- In order to calculate calculate AUC by using scikit-learn, the probabilities above are needed. \n",
        "- ```sklearn.metrics.roc_auc_score``` is a useful tool to obtain the auc value\n",
        "- On the other hand, if a figure of the ROC curve is required then ```sklearn.metrics .plot_roc_curve``` can be used. Let's see how: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppnDRYYsun4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b513452e-fbda-4a05-d57c-203d4ebc2e8f"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
        "\n",
        "#calculate auc\n",
        "auc = roc_auc_score(y_test, probs[:,1])\n",
        "print(\"AUC Score for Malignant is:\" , auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score for Malignant is: 0.990070093457944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53wlMmbQ8V2i"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC4yBf9Bun4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c9ae754f-afdc-4153-ab1c-f5cd9f443cd3"
      },
      "source": [
        "#plot roc curve\n",
        "plot_roc_curve(lr, X_test,y_test)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xV1Zn/8c9DAEFAoIC/qoBBhKlcBGsqZRBBqogUoSgVsVZxUIcq3rAWOjIF0ak6MpTaMlVExksrFBVqigjqTBCsVS4awk0xKmIAC0VBKEWJPL8/9k56yPWEZJ9Dsr/v1+u82Je193l2Qs5z1lp7r2XujoiIxFe9dAcgIiLppUQgIhJzSgQiIjGnRCAiEnNKBCIiMVc/3QFUVevWrT0zMzPdYYiI1Cpr1qz5q7u3KWtfrUsEmZmZrF69Ot1hiIjUKmb2UXn71DQkIhJzSgQiIjGnRCAiEnNKBCIiMadEICISc5ElAjObY2Y7zWx9OfvNzB4ys3wzyzOzb0YVi4iIlC/KGsHjwKAK9l8MdApfNwC/iTAWEREpR2TPEbj7cjPLrKDIMOBJD8bBfsPMWpjZSe6+I6qYarOn39zK87nb0h2GiKRRl5NPYPIlXWv8vOnsIzgF+DhhvSDcVoqZ3WBmq81s9a5du1IS3LHm+dxtbNzxebrDEJE6qFY8Wezus4BZAFlZWbV6Jp2j/Wa/ccfndDnpBH7/r70jiEpE4iydNYJtQLuE9bbhtjrtaL/ZdznpBIb1LLPCJCJSLemsEWQD48xsHtAL2BuX/gF9sxeRY0lkicDM5gL9gdZmVgBMBhoAuPvDwGJgMJAPHACujSoWEREpX5R3DY2qZL8DN0X1/iIikhw9WSwiEnO14q6h2iDZu4GK7v4RETlWqEZQQ5K9G0h3/4jIsUY1ghqku4FEpDZSIjgKZTUDqclHRGorNQ0dhbKagdTkIyK1lWoER0nNQCJSV6hGICISc6oRJKFkn4D6A0SkLlGNIAkl+wTUHyAidYlqBElSn4CI1FWqEYiIxJwSgYhIzCkRiIjEnBKBiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzMX6gTLNKiYiEvMagWYVExGJUY2goslkNHSEiMRZbGoEmkxGRKRssakRgAaOExEpS2xqBCIiUjYlAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiblIE4GZDTKzd80s38wmlrG/vZnlmNnbZpZnZoOjjEdEREqLLBGYWQYwE7gY6AKMMrMuJYpNAua7+1nAFcB/RxWPiIiULcoawTlAvrt/4O5fAvOAYSXKOFA0rGdzYHuE8YiISBmiTASnAB8nrBeE2xJNAa4yswJgMXBzWScysxvMbLWZrd61a1cUsYqIxFa6O4tHAY+7e1tgMPCUmZWKyd1nuXuWu2e1adMm5UGKiNRlUSaCbUC7hPW24bZEY4D5AO7+Z6AR0DrCmEREpIQoE8EqoJOZdTCzhgSdwdklymwFvgNgZmcQJAK1/YiIpFBkicDdC4FxwFJgE8HdQRvMbKqZDQ2L3QFcb2ZrgbnAaHf3qGISEZHSIp2PwN0XE3QCJ277WcLyRqBPlDGIiEjF0t1ZLCIiaaZEICISc0oEIiIxp0QgIhJzSgQiIjGnRCAiEnNKBCIiMadEICISc0oEIiIxp0QgIhJzSScCMzs+ykBERCQ9Kk0EZvbPZrYReCdc72FmmlJSRKSOSKZG8AvgImA3gLuvBc6LMigREUmdpJqG3P3jEpu+iiAWERFJg2SGof7YzP4ZcDNrANxKML+AiIjUAcnUCMYCNxFMPL8N6AncGGVQIiKSOsnUCP7J3X+QuMHM+gB/iiYkERFJpWRqBL9KcpuIiNRC5dYIzKw38M9AGzMbn7DrBCAj6sBERCQ1Kmoaagg0Dcs0S9j+OTAiyqBERCR1yk0E7v4q8KqZPe7uH6UwJhERSaFkOosPmNmDQFegUdFGdx8QWVQiIpIyyXQW/45geIkOwN3AFmBVhDGJiEgKJZMIWrn7Y8Ahd3/V3f8FUG1ARKSOSKZp6FD47w4z+y6wHfhadCGJiEgqJZMI7jWz5sAdBM8PnADcFmlUIiKSMpUmAndfFC7uBc6H4ieLRUSkDqjogbIM4HKCMYaWuPt6MxsC/BvQGDgrNSGKiEiUKqoRPAa0A1YCD5nZdiALmOjuf0hFcCIiEr2KEkEWcKa7HzazRsAnQEd3352a0EREJBUqun30S3c/DODuB4EPqpoEzGyQmb1rZvlmNrGcMpeb2UYz22BmT1fl/CIiUn0V1Qi+YWZ54bIBHcN1A9zdz6zoxGEfw0zgQqAAWGVm2e6+MaFMJ+CnQB93/8zMTqzGtYiIyFGoKBGcUc1znwPku/sHAGY2DxgGbEwocz0w090/A3D3ndV8TxERqaKKBp2r7kBzpwCJcx0XAL1KlOkMYGZ/Ihjaeoq7Lyl5IjO7AbgBoH379tUMS0REEiU1eX2E6gOdgP7AKOBRM2tRspC7z3L3LHfPatOmTYpDFBGp26JMBNsIbj8t0jbclqgAyHb3Q+7+IbCZIDGIiEiKJJUIzKyxmf1TFc+9CuhkZh3MrCFwBZBdoswfCGoDmFlrgqaiD6r4PiIiUg2VJgIzuwTIBZaE6z3NrOQHeinuXgiMA5YCm4D57r7BzKaa2dCw2FJgt5ltBHKAO/WcgohIaiUz6NwUgjuAlgG4e66ZdUjm5O6+GFhcYtvPEpYdGB++REQkDZJpGjrk7ntLbPMoghERkdRLpkawwcyuBDLCB8BuAV6PNiwREUmVZGoENxPMV/wF8DTBcNSaj0BEpI5IpkbwDXe/C7gr6mBERCT1kqkR/JeZbTKze8ysW+QRiYhISlWaCNz9fIKZyXYBj5jZOjObFHlkIiKSEkk9UObun7j7Q8BYgmcKflbJISIiUksk80DZGWY2xczWEUxe/zrBcBEiIlIHJNNZPAf4PXCRu2+POB4REUmxShOBu/dORSAiIpIe5SYCM5vv7peHTUKJTxInNUOZiIjUDhXVCG4N/x2SikBERCQ9yu0sdvcd4eKN7v5R4gu4MTXhiYhI1JK5ffTCMrZdXNOBiIhIelTUR/Ajgm/+p5lZXsKuZsCfog5MRERSo6I+gqeBF4H7gIkJ2/e5+6eRRiUiIilTUSJwd99iZjeV3GFmX1MyEBGpGyqrEQwB1hDcPmoJ+xw4LcK4REQkRcpNBO4+JPw3qWkpRUSkdkpmrKE+ZtYkXL7KzKabWfvoQxMRkVRI5vbR3wAHzKwHcAfwPvBUpFGJiEjKJJMICt3dgWHAr919JsEtpCIiUgckM/roPjP7KfBDoK+Z1QMaRBuWiIikSjI1gpEEE9f/i7t/QjAXwYORRiUiIimTzFSVnwC/A5qb2RDgoLs/GXlkIiKSEsncNXQ5sBL4PnA58KaZjYg6MBERSY1k+gjuAr7l7jsBzKwN8ArwbJSBiYhIaiTTR1CvKAmEdid5nIiI1ALJ1AiWmNlSYG64PhJYHF1IIiKSSsnMWXynmV0KnBtumuXuC6MNS0REUqWi+Qg6AdOAjsA64Mfuvi1VgYmISGpU1NY/B1gEXEYwAumvqnpyMxtkZu+aWb6ZTayg3GVm5maWVdX3EBGR6qmoaaiZuz8aLr9rZm9V5cRmlgHMJJjqsgBYZWbZ7r6xRLlmwK3Am1U5v4iI1IyKEkEjMzuLf8xD0Dhx3d0rSwznAPnu/gGAmc0jGK9oY4ly9wAPAHdWMXYREakBFSWCHcD0hPVPEtYdGFDJuU8BPk5YLwB6JRYws28C7dz9BTMrNxGY2Q3ADQDt22sEbBGRmlTRxDTnR/nG4eB104HRlZV191nALICsrCyPMi4RkbiJ8sGwbUC7hPW24bYizYBuwDIz2wJ8G8hWh7GISGpFmQhWAZ3MrIOZNQSuALKLdrr7Xndv7e6Z7p4JvAEMdffVEcYkIiIlRJYI3L0QGAcsBTYB8919g5lNNbOhUb2viIhUTaVPFpuZAT8ATnP3qeF8xV9395WVHevuiykxHIW7/6ycsv2TilhERGpUMjWC/wZ6A6PC9X0EzweIiEgdkMygc73c/Ztm9jaAu38WtvmLiEgdkEyN4FD4lLBD8XwEhyONSkREUiaZRPAQsBA40cz+A3gN+HmkUYmISMokMwz178xsDfAdguElvufumyKPTEREUiKZu4baAweAPyZuc/etUQYmIiKpkUxn8QsE/QMGNAI6AO8CXSOMS0REUiSZpqHuievhQHE3RhaRiIikVJWfLA6Hn+5VaUEREakVkukjGJ+wWg/4JrA9sohERCSlkukjaJawXEjQZ/BcNOGIiEiqVZgIwgfJmrn7j1MUj4iIpFi5fQRmVt/dvwL6pDAeERFJsYpqBCsJ+gNyzSwbeAb4W9FOd18QcWwiIpICyfQRNAJ2E8xRXPQ8gQNKBCIidUBFieDE8I6h9fwjARTRvMEiInVERYkgA2jKkQmgiBKBiEgdUVEi2OHuU1MWiYiIpEVFTxaXVRMQEZE6pqJE8J2URSEiImlTbiJw909TGYiIiKRHlQedExGRukWJQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiLtJEYGaDzOxdM8s3s4ll7B9vZhvNLM/M/tfMTo0yHhERKS2yRBDOdzwTuBjoAowysy4lir0NZLn7mcCzwH9GFY+IiJQtyhrBOUC+u3/g7l8C84BhiQXcPcfdD4SrbwBtI4xHRETKEGUiOAX4OGG9INxWnjHAi2XtMLMbzGy1ma3etWtXDYYoIiLHRGexmV0FZAEPlrXf3We5e5a7Z7Vp0ya1wYmI1HHJTF5/tLYB7RLW24bbjmBmFwB3Af3c/YsI4xERkTJEWSNYBXQysw5m1hC4AshOLGBmZwGPAEPdfWeEsYiISDkiSwTuXgiMA5YCm4D57r7BzKaa2dCw2INAU+AZM8s1s+xyTiciIhGJsmkId18MLC6x7WcJyxdE+f4iIlK5Y6KzWERE0keJQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERirn66AxCpjkOHDlFQUMDBgwfTHYrIMaFRo0a0bduWBg0aJH2MEoHUagUFBTRr1ozMzEzMLN3hiKSVu7N7924KCgro0KFD0sepaUhqtYMHD9KqVSslARHAzGjVqlWVa8hKBFLrKQmI/MPR/D0oEYiIxJwSgUg1NW3atNrnWL16Nbfccku5+7ds2cLTTz+ddHmAzMxMunfvzplnnkm/fv346KOPqh1nTXn44Yd58skna+RcO3bsYMiQIUdsu+222zjllFM4fPhw8bYpU6Ywbdq0I8plZmby17/+FYBPPvmEK664go4dO3L22WczePBgNm/eXK3YvvjiC0aOHMnpp59Or1692LJlS5nlfvnLX9KtWze6du3KjBkzirevXbuW3r170717dy655BI+//xzANatW8fo0aOrFVsiJQKRY0BWVhYPPfRQuftLJoLKyhfJyckhLy+P/v37c++991Y7Tnc/4sP1aI0dO5arr7662ucBmD59Otdff33x+uHDh1m4cCHt2rXj1VdfTeoc7s7w4cPp378/77//PmvWrOG+++7jL3/5S7Vie+yxx2jZsiX5+fncfvvtTJgwoVSZ9evX8+ijj7Jy5UrWrl3LokWLyM/PB+C6667j/vvvZ926dQwfPpwHH3wQgO7du1NQUMDWrVurFV8R3TUkdcbdf9zAxu2f1+g5u5x8ApMv6Vrl43Jzcxk7diwHDhygY8eOzJkzh5YtW7Jq1SrGjBlDvXr1uPDCC3nxxRdZv349y5YtY9q0aSxatIhXX32VW2+9FQjae5cvX87EiRPZtGkTPXv25JprruGss84qLr9//35uvvlmVq9ejZkxefJkLrvssiPi6d27d3Hi2LVrF2PHji3+EJkxYwZ9+vRh165dXHnllWzfvp3evXvz8ssvs2bNGvbv389FF11Er169WLNmDYsXL2b+/PnMnz+fL774guHDh3P33Xfzt7/9jcsvv5yCggK++uor/v3f/52RI0cyceJEsrOzqV+/PgMHDmTatGlMmTKFpk2b8uMf/7jcn1X//v3p1asXOTk57Nmzh8cee4y+ffuW+lk/99xzRyS5ZcuW0bVrV0aOHMncuXM5//zzK/195eTk0KBBA8aOHVu8rUePHlX+vZf0/PPPM2XKFABGjBjBuHHjcPcj2vE3bdpEr169OP744wHo168fCxYs4Cc/+QmbN2/mvPPOA+DCCy/koosu4p577gHgkksuYd68efzkJz+pdpyqEYhE4Oqrr+aBBx4gLy+P7t27c/fddwNw7bXX8sgjj5Cbm0tGRkaZx06bNo2ZM2eSm5vLihUraNy4Mffffz99+/YlNzeX22+//Yjy99xzD82bN2fdunXk5eUxYMCAUudcsmQJ3/ve9wC49dZbuf3221m1ahXPPfcc1113HQB33303AwYMYMOGDYwYMeKIb5vvvfceN954Ixs2bODdd9/lvffeY+XKleTm5rJmzRqWL1/OkiVLOPnkk1m7di3r169n0KBB7N69m4ULF7Jhwwby8vKYNGlS0j8rgMLCQlauXMmMGTOO2F7kww8/pGXLlhx33HHF2+bOncuoUaMYPnw4L7zwAocOHSr391Rk/fr1nH322ZWWA+jbty89e/Ys9XrllVdKld22bRvt2rUDoH79+jRv3pzdu3cfUaZbt26sWLGC3bt3c+DAARYvXszHH38MQNeuXXn++ecBeOaZZ4q3Q1ArXLFiRVIxV0Y1AqkzjuabexT27t3Lnj176NevHwDXXHMN3//+99mzZw/79u2jd+/eAFx55ZUsWrSo1PF9+vRh/Pjx/OAHP+DSSy+lbdu2Fb7fK6+8wrx584rXW7ZsWbx8/vnn8+mnn9K0adPib5KvvPIKGzduLC7z+eefs3//fl577TUWLlwIwKBBg444z6mnnsq3v/1tAF566SVeeuklzjrrLAD279/Pe++9R9++fbnjjjuYMGECQ4YMoW/fvhQWFtKoUSPGjBnDkCFDSrXll/ezKnLppZcCcPbZZ5fZvr5jxw7atGlTvP7ll1+yePFipk+fTrNmzejVqxdLly5lyJAh5d5NU9W7bGrqw7fIGWecwYQJExg4cCBNmjShZ8+exV8S5syZwy233MI999zD0KFDadiwYfFxJ554Itu3b6+RGCKtEZjZIDN718zyzWxiGfuPM7Pfh/vfNLPMKOMRqQ0mTpzI7Nmz+fvf/06fPn145513jvpcOTk5fPTRR/Ts2ZPJkycDQRv6G2+8QW5uLrm5uWzbtq3SDu8mTZoUL7s7P/3pT4uPz8/PZ8yYMXTu3Jm33nqL7t27M2nSJKZOnUr9+vVZuXIlI0aMYNGiRQwaNKhK8Rd908/IyKCwsLDU/saNGx9xz/zSpUvZs2cP3bt3JzMzk9dee425c+cC0KpVKz777LMjjt+3bx8tWrSga9eurFmzJqmYqlIjOOWUU4q/xRcWFrJ3715atWpVqtyYMWOKa1YtW7akc+fOAHzjG9/gpZdeYs2aNYwaNYqOHTsWH3Pw4EEaN26cVMyViSwRmFkGMBO4GOgCjDKzLiWKjQE+c/fTgV8AD0QVj0iqNG/enJYtWxZ/c3zqqafo168fLVq0oFmzZrz55psAR3yLT/T+++/TvXt3JkyYwLe+9S3eeecdmjVrxr59+8osf+GFFzJz5szi9ZIfdvXr12fGjBk8+eSTfPrppwwcOJBf/epXxftzc3OBoCYyf/58IPjWX/I8RS666CLmzJnD/v37gaD5Y+fOnWzfvp3jjz+eq666ijvvvJO33nqL/fv3s3fvXgYPHswvfvEL1q5dm9TPKlmdO3c+oqYwd+5cZs+ezZYtW9iyZQsffvghL7/8MgcOHOC8884jOzu7+Oe4YMECevToQUZGBgMGDOCLL75g1qxZxefKy8sr89v/ihUripNg4uuCCy4oVXbo0KE88cQTADz77LMMGDCgzBrIzp07Adi6dSsLFizgyiuvPGL74cOHuffee4/ow9i8eTPdunVL+mdVkSibhs4B8t39AwAzmwcMAzYmlBkGTAmXnwV+bWbm7h5hXCI16sCBA0c034wfP54nnniiuAP0tNNO43/+53+A4C6S66+/nnr16tGvXz+aN29e6nwzZswgJyeHevXq0bVrVy6++GLq1atHRkYGPXr0YPTo0cXNMgCTJk3ipptuolu3bmRkZDB58uTiJpUiJ510EqNGjWLmzJk89NBD3HTTTZx55pkUFhZy3nnn8fDDDzN58mRGjRrFU089Re/evfn6179Os2bNij/wiwwcOJBNmzYVN3E1bdqU3/72t+Tn53PnnXdSr149GjRowG9+8xv27dvHsGHDOHjwIO7O9OnTS11veT+rZDRp0oSOHTuSn5/PySefzJIlS3j44YeP2H/uuefyxz/+kZEjRzJu3DjOPfdczIwTTzyR2bNnA0Hz0MKFC7ntttt44IEHaNSoEZmZmUfcynk0xowZww9/+ENOP/10vva1rxUn/+3bt3PdddexePFiAC677DJ2795NgwYNmDlzJi1atACCxFaU5C+99FKuvfba4nPn5OTw3e9+t1rxFXP3SF7ACGB2wvoPgV+XKLMeaJuw/j7Quoxz3QCsBla3b9/ej8aU7PU+JXv9UR0rx66NGzemO4Qq2bdvX/Hyfffd57fccksaoznSwYMH/dChQ+7u/vrrr3uPHj3SHFFyFixY4HfddVe6w0ipgwcPeq9evYp/XyWV9XcBrPZyPq9rRWexu88CZgFkZWUdVW3hWOlIlHh74YUXuO+++ygsLOTUU0/l8ccfT3dIxbZu3crll1/O4cOHadiwIY8++mi6Q0rK8OHDS92JU9dt3bqV+++/n/r1a+YjPMpEsA1ol7DeNtxWVpkCM6sPNAfi9RuVWBk5ciQjR45Mdxhl6tSpE2+//Xa6wzgqRbfAxkWnTp3o1KlTjZ0vyruGVgGdzKyDmTUErgCyS5TJBq4Jl0cA/xdWYUSSpv8yIv9wNH8PkSUCdy8ExgFLgU3AfHffYGZTzWxoWOwxoJWZ5QPjgVK3mIpUpFGjRuzevVvJQIR/zEfQqFGjKh1nte0PKCsry1evXp3uMOQYoRnKRI5U3gxlZrbG3bPKOqZWdBaLlKdBgwZVmolJRErTWEMiIjGnRCAiEnNKBCIiMVfrOovNbBdwtFMttQb+WoPh1Aa65njQNcdDda75VHdvU9aOWpcIqsPMVpfXa15X6ZrjQdccD1Fds5qGRERiTolARCTm4pYIZlVepM7RNceDrjkeIrnmWPURiIhIaXGrEYiISAlKBCIiMVcnE4GZDTKzd80s38xKjWhqZseZ2e/D/W+aWWbqo6xZSVzzeDPbaGZ5Zva/ZnZqOuKsSZVdc0K5y8zMzazW32qYzDWb2eXh73qDmT2d6hhrWhL/t9ubWY6ZvR3+/x6cjjhripnNMbOdZra+nP1mZg+FP488M/tmtd+0vKnLausLyCCY8vI0oCGwFuhSosyNwMPh8hXA79Mddwqu+Xzg+HD5R3G45rBcM2A58AaQle64U/B77gS8DbQM109Md9wpuOZZwI/C5S7AlnTHXc1rPg/4JrC+nP2DgRcBA74NvFnd96yLNYJzgHx3/8DdvwTmAcNKlBkGPBEuPwt8x8wshTHWtEqv2d1z3P1AuPoGwYxxtVkyv2eAe4AHgLowTnUy13w9MNPdPwNw950pjrGmJXPNDpwQLjcHtqcwvhrn7suBTysoMgx40gNvAC3M7KTqvGddTASnAB8nrBeE28os48EEOnuBVimJLhrJXHOiMQTfKGqzSq85rDK3c/cXUhlYhJL5PXcGOpvZn8zsDTMblLLoopHMNU8BrjKzAmAxcHNqQkubqv69V0rzEcSMmV0FZAH90h1LlMysHjAdGJ3mUFKtPkHzUH+CWt9yM+vu7nvSGlW0RgGPu/t/mVlv4Ckz6+buh9MdWG1RF2sE24B2Cettw21lljGz+gTVyd0piS4ayVwzZnYBcBcw1N2/SFFsUansmpsB3YBlZraFoC01u5Z3GCfzey4Ast39kLt/CGwmSAy1VTLXPAaYD+DufwYaEQzOVlcl9fdeFXUxEawCOplZBzNrSNAZnF2iTDZwTbg8Avg/D3thaqlKr9nMzgIeIUgCtb3dGCq5Znff6+6t3T3T3TMJ+kWGunttnuc0mf/bfyCoDWBmrQmaij5IZZA1LJlr3gp8B8DMziBIBLtSGmVqZQNXh3cPfRvY6+47qnPCOtc05O6FZjYOWEpwx8Ecd99gZlOB1e6eDTxGUH3MJ+iUuSJ9EVdfktf8INAUeCbsF9/q7kPTFnQ1JXnNdUqS17wUGGhmG4GvgDvdvdbWdpO85juAR83sdoKO49G1+Yudmc0lSOatw36PyUADAHd/mKAfZDCQDxwArq32e9bin5eIiNSAutg0JCIiVaBEICISc0oEIiIxp0QgIhJzSgQiIjGnRCDHJDP7ysxyE16ZFZTdXwPv97iZfRi+11vhE6pVPcdsM+sSLv9biX2vVzfG8DxFP5f1ZvZHM2tRSfmetX00Tomebh+VY5KZ7Xf3pjVdtoJzPA4scvdnzWwgMM3dz6zG+aodU2XnNbMngM3u/h8VlB9NMOrquJqOReoO1QikVjCzpuE8Cm+Z2TozKzXSqJmdZGbLE74x9w23DzSzP4fHPmNmlX1ALwdOD48dH55rvZndFm5rYmYvmNnacPvIcPsyM8sys/uBxmEcvwv37Q//nWdm302I+XEzG2FmGWb2oJmtCseY/9ckfix/JhxszMzOCa/xbTN73cz+KXwSdyowMoxlZBj7HDNbGZYta8RWiZt0j72tl15lvQieis0NXwsJnoI/IdzXmuCpyqIa7f7w3zuAu8LlDILxhloTfLA3CbdPAH5Wxvs9DowIl78PvAmcDawDmhA8lb0BOAu4DHg04djm4b/LCOc8KIopoUxRjMOBJ8LlhgSjSDYGbgAmhZg0OtcAAAKDSURBVNuPA1YDHcqIc3/C9T0DDArXTwDqh8sXAM+Fy6OBXycc/3PgqnC5BcFYRE3S/fvWK72vOjfEhNQZf3f3nkUrZtYA+LmZnQccJvgm/P+ATxKOWQXMCcv+wd1zzawfwWQlfwqH1mhI8E26LA+a2SSCcWrGEIxfs9Dd/xbGsADoCywB/svMHiBoTlpRhet6EfilmR0HDAKWu/vfw+aoM81sRFiuOcFgcR+WOL6xmeWG178JeDmh/BNm1olgmIUG5bz/QGComf04XG8EtA/PJTGlRCC1xQ+ANsDZ7n7IghFFGyUWcPflYaL4LvC4mU0HPgNedvdRSbzHne7+bNGKmX2nrELuvtmCuQ4GA/ea2f+6+9RkLsLdD5rZMuAiYCTBRCsQzDZ1s7svreQUf3f3nmZ2PMH4OzcBDxFMwJPj7sPDjvVl5RxvwGXu/m4y8Uo8qI9AaovmwM4wCZwPlJpz2YJ5mP/i7o8Cswmm+3sD6GNmRW3+Tcysc5LvuQL4npkdb2ZNCJp1VpjZycABd/8twWB+Zc0ZeyismZTl9wQDhRXVLiD4UP9R0TFm1jl8zzJ5MNvcLcAd9o+h1IuGIh6dUHQfQRNZkaXAzRZWjywYlVZiTolAaovfAVlmtg64GninjDL9gbVm9jbBt+1fuvsugg/GuWaWR9As9I1k3tDd3yLoO1hJ0Gcw293fBroDK8MmmsnAvWUcPgvIK+osLuElgomBXvFg+kUIEtdG4C0LJi1/hEpq7GEseQQTs/wncF947YnH5QBdijqLCWoODcLYNoTrEnO6fVREJOZUIxARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARibn/D/FQQXVfWNV3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}