{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.9.Reinforcement Learning_MonteCarlo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITU-Business-Analytics-Team/Business_Analytics_for_Professionals/blob/main/Part%20I%20%3A%20Methods%20%26%20Technologies%20for%20Business%20Analytics/Chapter%203%3A%20Prediction%20Modelling/3_9_Reinforcement_Learning_MonteCarlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY5VxdspuqqI"
      },
      "source": [
        "# **Reinforcement Learning**\n",
        "## **Monte Carlo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-RJWWha6if2"
      },
      "source": [
        "### **Problem Definition**\n",
        "The well-known maze problem is used to demonstrate the use of Monte Carlo  algorithm. There is a ***n x n*** grid, which allows players to move in four directions: north, south, east, and west. Certain cells contain obtacles. The agent begins in one cell and works its way to the target cell. The objective is to determine the optimal sequence of motions that will result in the agent reaching the goal cell with the greatest reward.\n",
        "The Q-learning technique is employed in this case to solve this problem. The following steps in building the algorithm are as follows:\n",
        "1.   Define the environment as a class of objects in which agents operate.\n",
        "2. Define the agents as a class of objects.\n",
        "3. Finally, we trained our agent how to behave in the described environment  to achieve its purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhQCdw_Gm8NE"
      },
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrF0EXDum--m"
      },
      "source": [
        "#First define gridmaze environment as a class\n",
        "class Environment():\n",
        "  def __init__(self, height, width, blocked_cell, start_cell, end_cell, current_location):\n",
        "    #Define the dimensions of grid and cell features\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    self.blocked_cell = blocked_cell\n",
        "    self.end_cell = end_cell\n",
        "    self.start_cell = start_cell\n",
        "    self.current_location = current_location\n",
        "    self.state_space=[]\n",
        "    self.create_state_space()\n",
        "\n",
        "    #Define the immediate reward of each cell if the agent reach moves that cell\n",
        "    self.cell_reward = np.zeros((self.height, self.width))-1 #When agent moves from one cell to another, it gets -1 reward\n",
        "    self.cell_reward[self.end_cell[0], self.end_cell[1]] = 10 #if the agent reaches the end cell, gets 10 reward\n",
        "    self.cell_reward[self.blocked_cell[0], self.blocked_cell[1]] = -10 #if the agent tries to move the blocked cell, it gets -10 reward\n",
        "\n",
        "    self.actions = ['Left', 'Right', 'Up','Down'] # There are 4 possible actions of the agent.\n",
        "\n",
        "  def get_actions(self):\n",
        "      return self.actions\n",
        "  def create_state_space(self):    \n",
        "      for i in range(self.height):\n",
        "        for j in range(self.width):\n",
        "          if (i,j)!=self.blocked_cell:\n",
        "            self.state_space.append((i,j))    \n",
        "\n",
        "  def return_reward(self, next_location):\n",
        "     return self.cell_reward[next_location[0], next_location[1]]\n",
        "\n",
        "  def check_end_cell(self):\n",
        "      if self.current_location == self.end_cell:\n",
        "        return True\n",
        "  def check_blocked_cell(self):\n",
        "      if self.current_location == self.blocked_cell:\n",
        "        return True    \n",
        "  \n",
        "  def reset(self):\n",
        "    self.current_location = self.state_space[np.random.choice(len(self.state_space))]\n",
        "    while self.current_location==self.end_cell:\n",
        "      self.current_location = self.state_space[np.random.choice(len(self.state_space))]\n",
        "    \n",
        "  def step(self, action):\n",
        "        \"\"\"Directs the agent forward. If agent is at the border of or adjacent to a blocked cell, the agent's position is unaffected; however, \n",
        "        the agent takes a negative reward. Reward is returned by function.\"\"\"\n",
        "              \n",
        "        last_location = self.current_location\n",
        "        \n",
        "        # UP\n",
        "        if action == 'Up':\n",
        "            # If agent is at the top, stay still, collect reward\n",
        "            if last_location[0] == 0:\n",
        "                reward = self.return_reward(last_location)\n",
        "            elif (last_location[0]-1, last_location[1]) == self.blocked_cell:\n",
        "               reward = self.return_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0] - 1, self.current_location[1])\n",
        "                reward = self.return_reward(self.current_location)\n",
        "        \n",
        "        # DOWN\n",
        "        elif action == 'Down':\n",
        "            # If agent is at bottom, stay still, collect reward\n",
        "            if last_location[0] == self.height - 1:\n",
        "                reward = self.return_reward(last_location)\n",
        "            elif (last_location[0]+1, last_location[1]) == self.blocked_cell:\n",
        "                reward = self.return_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0] + 1, self.current_location[1])\n",
        "                reward = self.return_reward(self.current_location)\n",
        "            \n",
        "        # LEFT\n",
        "        elif action == 'Left':\n",
        "            # If agent is at the left, stay still, collect reward\n",
        "            if last_location[1] == 0:\n",
        "                reward = self.return_reward(last_location)\n",
        "            elif (last_location[0], last_location[1]-1) == self.blocked_cell:\n",
        "                reward = self.return_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0], self.current_location[1] - 1)\n",
        "                reward = self.return_reward(self.current_location)\n",
        "\n",
        "        # RIGHT\n",
        "        elif action == 'Right':\n",
        "            # If agent is at the right, stay still, collect reward\n",
        "            if last_location[1] == self.width - 1:\n",
        "                reward = self.return_reward(last_location)\n",
        "            elif (last_location[0], last_location[1]+1) == self.blocked_cell:\n",
        "                reward = self.return_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = ( self.current_location[0], self.current_location[1] + 1)\n",
        "                reward = self.return_reward(self.current_location)\n",
        "                \n",
        "        return reward"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN9eYJoCNch8"
      },
      "source": [
        "def play_episode(environment,max_steps_per_episode=20):\n",
        "    environment.reset()\n",
        "    s=environment.current_location\n",
        "    a = np.random.choice(environment.actions) # first action is uniformly random\n",
        "    # keep track of all states and rewards encountered\n",
        "    episodestates = [s]\n",
        "    episodeactions = [a]\n",
        "    episoderewards = [0]\n",
        "    steps = 0\n",
        "    while not environment.check_end_cell():\n",
        "        r = environment.step(a)\n",
        "        next_s = environment.current_location\n",
        "        # update states and rewards lists\n",
        "        episodestates.append(next_s)\n",
        "        episoderewards.append(r)\n",
        "        s=next_s\n",
        "        if not environment.check_end_cell():\n",
        "            a = policy[s]\n",
        "            episodeactions.append(a)\n",
        "        steps += 1\n",
        "        if steps >= max_steps_per_episode:\n",
        "            break\n",
        "    s = next_s\n",
        "    return episodestates, episodeactions,episoderewards"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjSOn8wLqGMW"
      },
      "source": [
        "environment = Environment(height=4, width=4, blocked_cell= (2,2), start_cell= (0,0), end_cell = (3,3), current_location = (0,0))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJqEqAp8PTG7"
      },
      "source": [
        "#Generate a random policy\n",
        "policy={}\n",
        "for s in environment.state_space:\n",
        "  environment.current_location=s\n",
        "  if not environment.check_end_cell():\n",
        "    if not environment.check_blocked_cell():\n",
        "      action = environment.actions[np.random.randint(0,len(environment.actions)-1)]\n",
        "      policy[s]=action"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQIHP4c7Pq1B"
      },
      "source": [
        "# initialize Q(s,a) and sample counts\n",
        "Q = {}\n",
        "sample_counts={}\n",
        "for s in environment.state_space:\n",
        "  environment.current_location=s\n",
        "  if not environment.check_end_cell(): # not a terminal state\n",
        "    Q[s] = {}\n",
        "    sample_counts[s] = {}\n",
        "    for a in environment.actions:\n",
        "      Q[s][a] = 0\n",
        "      sample_counts[s][a]=0 #will be used to compute average "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4mO-1fOtxm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0c39bf5-6935-4ccc-a5c1-935caaf7bec3"
      },
      "source": [
        "#Monte Carlo Method\n",
        "GAMMA=0.9\n",
        "# repeat until convergence\n",
        "deltas = []\n",
        "for it in range(100000):\n",
        "    if it % 1000 == 0:\n",
        "        print(it)\n",
        "\n",
        "    # generate an episode using pi\n",
        "    biggest_change = 0\n",
        "    episodestates, episodeactions, episoderewards = play_episode(environment,max_steps_per_episode=20)\n",
        "    # create a list of only state-action pairs for lookup\n",
        "    states_actions = list(zip(episodestates, episodeactions))\n",
        "\n",
        "    T = len(episodestates)\n",
        "    G = 0\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        # retrieve current s, a, r tuple\n",
        "        s = episodestates[t]\n",
        "        a = episodeactions[t]\n",
        "        # update G\n",
        "        G = episoderewards[t+1] + GAMMA * G\n",
        "\n",
        "        # check if we have already seen (s, a) (\"first-visit\")\n",
        "        if (s, a) not in states_actions[:t]:\n",
        "            old_q = Q[s][a]\n",
        "            sample_counts[s][a] += 1\n",
        "            lr = 1 / sample_counts[s][a]\n",
        "            Q[s][a] = old_q + lr * (G - old_q) #Remember the incremental implementation of taking averages\n",
        "\n",
        "            # update policy\n",
        "            maxValue = max(Q[s].values())\n",
        "            action = np.random.choice([k for k, v in Q[s].items() if v == maxValue])\n",
        "            policy[s] = action\n",
        "        # update delta\n",
        "            biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
        "        deltas.append(biggest_change)\n",
        "\n",
        "plt.plot(deltas)\n",
        "#plt.xscale('log')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqKz5nA1qKSf"
      },
      "source": [
        "# determine the policy from Q*\n",
        "policy = {}\n",
        "V = {}\n",
        "for s in environment.state_space:\n",
        "    if s!=environment.end_cell:\n",
        "      maxValue = max(Q[s].values())\n",
        "      action = np.random.choice([k for k, v in Q[s].items() if v == maxValue])\n",
        "      policy[s] = action\n",
        "      V[s] = maxValue"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1J5vgLUW2Oi"
      },
      "source": [
        "### **Results** \n",
        "The above figure demonstrates that after a few episodes, Monte Carlo simulation converges and our agent has learned how to behave optimally in the specified environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axoyQ2GFrHM1",
        "outputId": "ad3f05ad-5b87-45ff-aa77-c7b7ba056e3d"
      },
      "source": [
        "V"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): 1.8086276036002298,\n",
              " (0, 1): 3.1190349811870886,\n",
              " (0, 2): 4.578508046883691,\n",
              " (0, 3): 6.199054264487862,\n",
              " (1, 0): 3.1198398168060946,\n",
              " (1, 1): 4.575514713984178,\n",
              " (1, 2): 6.198340320969875,\n",
              " (1, 3): 7.9988887189423385,\n",
              " (2, 0): 4.578338033526144,\n",
              " (2, 1): 6.199351191450353,\n",
              " (2, 3): 10.0,\n",
              " (3, 0): 6.199535657428235,\n",
              " (3, 1): 7.9997178062631,\n",
              " (3, 2): 10.0}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4mmsuHUrfXj"
      },
      "source": [
        "#Function to print policy on a grid\n",
        "def print_policy(policy,rows,columns):\n",
        "    for i in range(rows):\n",
        "        print(\"---------------------------------------\")\n",
        "        for j in range(columns):\n",
        "              a = policy.get((i,j), ' ')\n",
        "              print(\"  %s  |\" % a, end=\"\")\n",
        "        print(\"\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W780EzPUsHj2",
        "outputId": "3a238023-e1e3-45af-f031-4a65d9d7216c"
      },
      "source": [
        "print_policy(policy,4,4)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "  Down  |  Down  |  Right  |  Down  |\n",
            "---------------------------------------\n",
            "  Down  |  Down  |  Right  |  Down  |\n",
            "---------------------------------------\n",
            "  Down  |  Down  |     |  Down  |\n",
            "---------------------------------------\n",
            "  Right  |  Right  |  Right  |     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNOGWfbNwGE1"
      },
      "source": [
        "#Function to print values on a grid\n",
        "def print_values(V, rows,columns):\n",
        "    for i in range(rows):\n",
        "        print(\"---------------------------\")\n",
        "        for j in range(columns):\n",
        "            v = V.get((i,j), 0)\n",
        "            if v >= 0:\n",
        "                print(\" %.2f|\" % v, end=\"\")\n",
        "            else:\n",
        "                print(\"%.2f|\" % v, end=\"\")\n",
        "        print(\"\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8SPLpzxzXY2",
        "outputId": "c69950db-e807-4688-93f4-d3e4733d5d71"
      },
      "source": [
        "print_values(V, 4,4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------\n",
            " 1.81| 3.12| 4.58| 6.20|\n",
            "---------------------------\n",
            " 3.12| 4.58| 6.20| 8.00|\n",
            "---------------------------\n",
            " 4.58| 6.20| 0.00| 10.00|\n",
            "---------------------------\n",
            " 6.20| 8.00| 10.00| 0.00|\n"
          ]
        }
      ]
    }
  ]
}