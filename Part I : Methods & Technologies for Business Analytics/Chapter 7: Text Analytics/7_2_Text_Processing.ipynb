{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "7.2.Text Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ITU-Business-Analytics-Team/Business_Analytics_for_Professionals/blob/main/Part%20I%20%3A%20Methods%20%26%20Technologies%20for%20Business%20Analytics/Chapter%207%3A%20Text%20Analytics/7_2_Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHZbxlu_lFWF"
      },
      "source": [
        "# **Text Analytics**\n",
        "## Text Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_g8iIaOkMVJ"
      },
      "source": [
        "#Install necessary libraries\n",
        "import string\n",
        "#Install nltk library\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "#nltk.download()\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "#Install textblob library\n",
        "#!pip install textblob\n",
        "from textblob import TextBlob\n",
        "from textblob import Word"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwFtS5xfkMVM"
      },
      "source": [
        "### 2.1\tData Cleansing\n",
        "Punctuation marks are cleaned during data cleansing as they do not contain any extra value or information for analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0sGUxcDEkMVO",
        "outputId": "26445509-17b1-45a2-bb24-ca053ba4cca9"
      },
      "source": [
        "#Removing Punctuations\n",
        "text = \"the paragrahps, need to be preprocessed.!\"\n",
        "for c in string.punctuation:\n",
        "    text= text.replace(c,\"\")\n",
        "text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the paragrahps need to be preprocessed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGDt3V5jkMVP"
      },
      "source": [
        "### 2.2\tStop Words Removal\n",
        "Common words provide little contextual information compared to other keywords. Therefore, these are removed from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77YWyeeGkd9c",
        "outputId": "ae90078e-ba0a-410d-c3e1-71092eb0b99d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gDZonstkkMVP",
        "outputId": "7a291e5c-16dc-4628-b866-9550d5ac9cc0"
      },
      "source": [
        "#Removing Stop Words\n",
        "stop = stopwords.words('english')\n",
        "text = \" \".join([x for x in text.split() if x not in stop])\n",
        "text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paragrahps need preprocessed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSDEqr-KkMVQ"
      },
      "source": [
        "### 2.3\tSpelling Correction \n",
        "Words written incorrectly or incompletely in the text can be corrected with the help of libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r5gynwbfkMVR",
        "outputId": "815feb50-bf7a-49a0-afee-3dcc70671e4d"
      },
      "source": [
        "#Spelling Correction\n",
        "text = str(TextBlob(text).correct())\n",
        "text"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paragraphs need preprocessed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df-i168QkMVS"
      },
      "source": [
        "### 2.4\tLemmatization\n",
        "By applying this operation, plural suffixes are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ZAQ3g2cIkMVS",
        "outputId": "cb9ab673-a4bb-4531-e244-fb461d275e0f"
      },
      "source": [
        "#Lemmatizing\n",
        "nltk.download('wordnet')\n",
        "text = \" \".join([Word(word).lemmatize() for word in text.split()])\n",
        "text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paragraph need preprocessed'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-i0qtMXkMVT"
      },
      "source": [
        "### 2.5\tStemming\n",
        "These operations are applied to find the root word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FFPlPaJ4kMVU",
        "outputId": "401db804-6516-47bd-c1e4-0b0e413b9563"
      },
      "source": [
        "#Stemming\n",
        "st = PorterStemmer()\n",
        "text = \" \".join([st.stem(word) for word in text.split()])\n",
        "text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paragraph need preprocess'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8gmDJQdkMVU"
      },
      "source": [
        "### 2.6\tTokenization\n",
        "This process means dividing the text into minimum meaningful units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOE-KqClkMVU",
        "outputId": "a8984cb8-49bc-49db-c138-5feac9a1e4de"
      },
      "source": [
        "#Tokenization\n",
        "#Using textblob\n",
        "nltk.download('punkt')\n",
        "tokens = TextBlob(text).words\n",
        "tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['paragraph', 'need', 'preprocess'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPeXHBjZ-hL4"
      },
      "source": [
        "**Results** \\\\\n",
        "After tokenizing the final phrase, root words as [“paragraph”, “need”, “preprocessed”] are obtained."
      ]
    }
  ]
}